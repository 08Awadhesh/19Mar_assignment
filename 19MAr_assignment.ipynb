{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15b1765-6e91-46d4-a769-4f43e15fe603",
   "metadata": {},
   "source": [
    "**Q1.** What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "**Answer**: \n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numeric features within a specific range. It rescales the values of a feature to a common range, typically between 0 and 1. The purpose of Min-Max scaling is to ensure that all the features contribute equally to the analysis and prevent any feature from dominating the others due to its original scale.\n",
    "\n",
    "The formula to perform Min-Max scaling on a feature is as follows:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where:\n",
    "\n",
    "\"value\" is the original value of a data point.\n",
    "\"min_value\" is the minimum value of the feature in the dataset.\n",
    "\"max_value\" is the maximum value of the feature in the dataset.\n",
    "Let's consider an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset containing the heights of individuals in centimeters. The original heights range from 150 cm to 190 cm. We want to apply Min-Max scaling to transform these heights to a range between 0 and 1.\n",
    "\n",
    "Original heights:\n",
    "\n",
    "Person A: 160 cm\n",
    "Person B: 180 cm\n",
    "\n",
    "Person C: 150 cm\n",
    "\n",
    "Person D: 190 cm\n",
    "\n",
    "To scale these values using Min-Max scaling, we calculate the minimum and maximum values:\n",
    "\n",
    "min_value = 150 cm\n",
    "\n",
    "max_value = 190 cm\n",
    "\n",
    "Now, we apply the Min-Max scaling formula to each height:\n",
    "\n",
    "Scaled heights:\n",
    "\n",
    "Person A: (160 - 150) / (190 - 150) = 0.25\n",
    "\n",
    "Person B: (180 - 150) / (190 - 150) = 0.75\n",
    "\n",
    "Person C: (150 - 150) / (190 - 150) = 0.00\n",
    "\n",
    "Person D: (190 - 150) / (190 - 150) = 1.00\n",
    "\n",
    "After applying Min-Max scaling, the heights are transformed to a range between 0 and 1. This normalization ensures that the heights are now comparable with other features in the dataset that might have different scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79020e-2f8b-405f-a707-bd747e338c15",
   "metadata": {},
   "source": [
    "**Q2.** What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "**Answer**:\n",
    "The Unit Vector technique, also known as vector normalization or feature scaling, is a data preprocessing method that rescales the feature values to have a unit norm. In this technique, each data point is divided by its magnitude, resulting in a vector with a length of 1. The purpose of unit vector scaling is to ensure that the magnitude of a feature does not influence the analysis or computations based on that feature.\n",
    "\n",
    "The formula to perform Unit Vector scaling on a feature is as follows:\n",
    "\n",
    "scaled_vector = vector / ||vector||\n",
    "\n",
    "where:\n",
    "\n",
    "\"vector\" represents the original feature vector.\n",
    "\"||vector||\" denotes the magnitude or Euclidean norm of the vector, which is calculated as the square root of the sum of squared elements.\n",
    "Let's consider an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose we have a dataset containing the following feature vectors:\n",
    "\n",
    "Vector A: [2, 4, 6]\n",
    "\n",
    "Vector B: [1, 3, 5]\n",
    "\n",
    "Vector C: [3, 6, 9]\n",
    "\n",
    "To scale these vectors using the Unit Vector technique, we calculate their magnitudes:\n",
    "\n",
    "||Vector A|| = sqrt(2^2 + 4^2 + 6^2) = sqrt(56) = 7.48\n",
    "\n",
    "||Vector B|| = sqrt(1^2 + 3^2 + 5^2) = sqrt(35) = 5.92\n",
    "\n",
    "||Vector C|| = sqrt(3^2 + 6^2 + 9^2) = sqrt(126) = 11.22\n",
    "\n",
    "Now, we apply the Unit Vector scaling formula to each vector:\n",
    "\n",
    "Scaled vectors:\n",
    "\n",
    "Vector A: [2/7.48, 4/7.48, 6/7.48] = [0.27, 0.54, 0.81]\n",
    "\n",
    "Vector B: [1/5.92, 3/5.92, 5/5.92] = [0.17, 0.51, 0.85]\n",
    "\n",
    "Vector C: [3/11.22, 6/11.22, 9/11.22] = [0.27, 0.54, 0.81]\n",
    "\n",
    "After applying Unit Vector scaling, all the feature vectors have a magnitude of 1, indicating that they are now unit vectors. This scaling technique ensures that the direction and relative relationships between the feature vectors are preserved while removing the influence of their magnitudes. Unlike Min-Max scaling, Unit Vector scaling does not focus on the range of values but rather on the normalization of the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a9be7-df34-4ce5-b252-e3a3971b4e0e",
   "metadata": {},
   "source": [
    "**Q3**. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "**Answer**:\n",
    "PCA (Principal Component Analysis) is a widely used technique in data analysis and dimensionality reduction. It is used to transform a dataset with a high-dimensional feature space into a lower-dimensional space while preserving the most important patterns or variations in the data. PCA achieves this by identifying the principal components, which are orthogonal linear combinations of the original features.\n",
    "\n",
    "The steps involved in PCA are as follows:\n",
    "\n",
    "**(I) Standardize the data:** If the features in the dataset have different scales, it is essential to standardize them to have zero mean and unit variance. This step ensures that all the features contribute equally to the PCA.\n",
    "\n",
    "**(II) Compute the covariance matrix:** The covariance matrix is computed based on the standardized features. It represents the relationships and dependencies between the features in the dataset.\n",
    "\n",
    "**(III) Perform eigen decomposition**: The eigen decomposition is performed on the covariance matrix to obtain the eigenvalues and eigenvectors. The eigenvalues represent the amount of variance explained by each eigenvector, and the eigenvectors represent the directions or axes of maximum variance in the data.\n",
    "\n",
    "**(IV) Select the principal components**: The eigenvectors associated with the highest eigenvalues are selected as the principal components. These components capture the most significant patterns or variations in the data.\n",
    "\n",
    "**(VII) Project the data onto the principal components**: The original data is projected onto the selected principal components to obtain the transformed lower-dimensional representation. Each data point is represented by its coordinates along the principal components.\n",
    "\n",
    "Let's consider an example to illustrate the application of PCA:\n",
    "\n",
    "Suppose we have a dataset with two features, \"X\" and \"Y,\" representing the coordinates of points in a 2D space. We want to apply PCA to reduce the dimensionality of the data from 2D to 1D.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "Point 1: (2, 4)\n",
    "\n",
    "Point 2: (4, 2)\n",
    "\n",
    "Point 3: (6, 6)\n",
    "\n",
    "Steps:\n",
    "\n",
    "Standardize the data: We calculate the mean and standard deviation of the features \"X\" and \"Y\" and standardize the data.\n",
    "\n",
    "Compute the covariance matrix: Based on the standardized features, we calculate the covariance matrix.\n",
    "\n",
    "Perform eigen decomposition: We perform eigen decomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "Select the principal components: We select the eigenvector with the highest eigenvalue as the principal component.\n",
    "\n",
    "Project the data onto the principal component: We project each data point onto the principal component to obtain the transformed 1D representation.\n",
    "\n",
    "After applying PCA, we obtain the transformed dataset:\n",
    "\n",
    "Transformed dataset:\n",
    "\n",
    "Point 1: -2.12\n",
    "\n",
    "Point 2: 2.12\n",
    "\n",
    "Point 3: 0.00\n",
    "\n",
    "The transformed dataset represents the original data projected onto the principal component. The dimensionality has been reduced from 2D to 1D, while retaining the most significant patterns or variations in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab095fce-94ac-42e4-a2dd-e5b8951765f2",
   "metadata": {},
   "source": [
    "**Q4.** What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "**Answer**: \n",
    "PCA and feature extraction are closely related concepts. PCA can be used as a technique for feature extraction, where it transforms the original high-dimensional feature space into a lower-dimensional space by identifying the most important patterns or variations in the data.\n",
    "\n",
    "In feature extraction, the goal is to find a smaller set of representative features that capture the essential information from the original features. These representative features are often referred to as \"latent variables\" or \"new features.\" PCA helps in achieving this by finding linear combinations of the original features, called principal components, which explain the maximum variance in the data.\n",
    "\n",
    "By using PCA for feature extraction, we can reduce the dimensionality of the dataset while retaining the most significant information. The new set of features, derived from the principal components, can often capture the key characteristics of the data and be used in subsequent analysis or modeling tasks.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with five features, \"A,\" \"B,\" \"C,\" \"D,\" and \"E.\" Each feature represents a different aspect of a product. We want to extract a reduced set of features that capture the most important information.\n",
    "\n",
    "Using PCA for feature extraction:\n",
    "\n",
    "Standardize the data: We standardize the original features to have zero mean and unit variance.\n",
    "\n",
    "Perform PCA: We apply PCA to the standardized data. PCA calculates the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Select principal components: We select a subset of principal components based on their corresponding eigenvalues. The principal components with higher eigenvalues capture more variance in the data.\n",
    "\n",
    "Project the data onto the selected principal components: We project the original data onto the selected principal components to obtain the transformed features.\n",
    "\n",
    "For example, let's say PCA suggests that the first three principal components capture most of the variance in the data. We can choose these three principal components as the extracted features.\n",
    "\n",
    "The original dataset:\n",
    "\n",
    "Data point 1: [1, 2, 3, 4, 5]\n",
    "\n",
    "Data point 2: [2, 3, 4, 5, 6]\n",
    "\n",
    "Data point 3: [3, 4, 5, 6, 7]\n",
    "\n",
    "Transformed dataset (extracted features):\n",
    "\n",
    "Data point 1: [0.87, -0.13, 0.32]\n",
    "\n",
    "Data point 2: [0.29, -0.13, -0.47]\n",
    "\n",
    "Data point 3: [-0.29, -0.13, -1.26]\n",
    "\n",
    "In this example, PCA has extracted a reduced set of features (three features) from the original dataset (five features) while preserving the most important information. These extracted features can be used in subsequent analysis, such as clustering or classification tasks, with reduced dimensionality and improved computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03796a7d-23eb-4986-a39f-df9adcc13578",
   "metadata": {},
   "source": [
    "**Q5.** You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "**Answer**:To preprocess the dataset for building a recommendation system for a food delivery service, you can utilize Min-Max scaling to transform the features such as price, rating, and delivery time. Here's how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "**(I) Understand the range of each feature:** Analyze the range and distribution of each feature in the dataset, including price, rating, and delivery time. Determine the minimum and maximum values for each feature.\n",
    "\n",
    "**(II) Apply Min-Max scaling**: Once you have identified the minimum and maximum values for each feature, apply Min-Max scaling individually to normalize the values within a common range, typically between 0 and 1.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "Let's consider each feature and apply Min-Max scaling:\n",
    "\n",
    "Price: If the original prices range from $5 to $20, and you want to scale them to a range between 0 and 1, apply the Min-Max scaling formula to each price value.\n",
    "Rating: If the ratings range from 1 to 5, apply the Min-Max scaling formula to each rating value.\n",
    "Delivery Time: If the delivery times range from 30 minutes to 60 minutes, apply the Min-Max scaling formula to each delivery time value.\n",
    "By applying Min-Max scaling to each feature, you ensure that all the features are on the same scale, with values between 0 and 1.\n",
    "\n",
    "**(III) Normalize new data points:** If you receive new data points during the recommendation system's operation, make sure to apply the same Min-Max scaling technique used during preprocessing. Use the previously calculated minimum and maximum values for each feature to scale the new data points consistently.\n",
    "\n",
    "Min-Max scaling in this context allows you to normalize the features such as price, rating, and delivery time to a common range, ensuring that no single feature dominates the recommendation process due to its original scale. It helps to make the features comparable and contributes to building an effective recommendation system that considers multiple factors in a balanced manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d4b8e-e1ed-4c9b-a6eb-1f993801f7a0",
   "metadata": {},
   "source": [
    "**Q6.** You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "**Answer**:\n",
    "When building a model to predict stock prices with a dataset containing numerous features, such as company financial data and market trends, PCA (Principal Component Analysis) can be employed to reduce the dimensionality of the dataset. Here's an overview of the steps involved in using PCA for dimensionality reduction in this scenario:\n",
    "\n",
    "**(I) Data preprocessing:** Prior to applying PCA, it is essential to preprocess the data. This typically involves handling missing values, performing feature scaling, and addressing any other necessary data cleaning steps. Standardizing the features to have zero mean and unit variance is particularly important for PCA.\n",
    "\n",
    "**(II) Feature selection:** If there are irrelevant or redundant features in the dataset, it is beneficial to perform feature selection before applying PCA. This step helps in removing features that do not contribute much to the overall variation in the data or are highly correlated with other features.\n",
    "\n",
    "**(III) Apply PCA:** Once the data is preprocessed and feature selection (if applicable) is performed, PCA can be applied to the remaining features. PCA calculates the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "**(IV) Determine the number of components**: The number of principal components to retain depends on the desired level of dimensionality reduction. One common approach is to select a number of components that explain a significant portion of the total variance in the dataset, such as retaining components that capture, for example, 90% or 95% of the variance.\n",
    "\n",
    "**(V) Dimensionality reduction**: After determining the number of components, the dataset is transformed by projecting the original features onto the selected principal components. This results in a reduced-dimensional representation of the data, with each data point represented by its coordinates along the retained principal components.\n",
    "\n",
    "**(VI) Model building**: The reduced-dimensional dataset obtained from PCA can then be used as input for training a predictive model to forecast stock prices. The reduced dimensionality helps in mitigating the curse of dimensionality, improving model training efficiency, and potentially reducing the risk of overfitting.\n",
    "\n",
    "By using PCA for dimensionality reduction in the context of predicting stock prices, you can effectively capture the most significant patterns or variations in the dataset while reducing the number of features. This enables the model to focus on the most informative aspects of the data, leading to potentially improved prediction accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858dc9aa-06c9-4bec-9b95-27bf9ec3ff36",
   "metadata": {},
   "source": [
    "**Q7.** For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20fe0dc5-3f53-4667-a8d0-feaccab4ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7ec8320-b553-4de3-b6bc-802334067ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38e81940-cf56-49c8-9557-d70c384ee8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa0823f5-cb4a-40bf-9ef7-8fad258a8bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.21052632],\n",
       "       [0.47368421],\n",
       "       [0.73684211],\n",
       "       [1.        ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform([[i] for i in lst])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b62e9-3167-4ce2-92fb-461a64fd8817",
   "metadata": {},
   "source": [
    "**Q8**. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "**Answer**:To determine the number of principal components to retain during PCA (Principal Component Analysis), we need to consider the explained variance ratio of each principal component. The explained variance ratio tells us how much information or variance is explained by each principal component.\n",
    "\n",
    "Here's an example of\n",
    "how you can perform feature extraction using PCA in Python and analyze the explained variance ratio to decide how many principal components to retain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45085f40-aabe-4759-b4ac-d5b91654d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n",
      "[9.30760167e-01 6.84463380e-02 7.93495226e-04 1.82435065e-32]\n",
      "\n",
      "Cumulative Explained Variance:\n",
      "[0.93076017 0.9992065  1.         1.        ]\n",
      "\n",
      "Number of Principal Components to Retain: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([\n",
    "    [170, 65, 30, 0, 120],\n",
    "    [165, 60, 35, 1, 130],\n",
    "    [180, 75, 40, 1, 140],\n",
    "    [160, 55, 28, 0, 115],\n",
    "    [175, 70, 32, 1, 125]\n",
    "])\n",
    "\n",
    "# Separate features from the dataset\n",
    "features = data[:, :4]\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "\n",
    "# Get the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of principal components to retain\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# Print the explained variance ratio and cumulative explained variance\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "print(\"\\nCumulative Explained Variance:\")\n",
    "print(cumulative_variance)\n",
    "print(\"\\nNumber of Principal Components to Retain:\", n_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb44811-36c7-449e-8b46-d2ff9c824176",
   "metadata": {},
   "source": [
    "In the above code, we use the PCA class from the sklearn.decomposition module to perform PCA. We fit the PCA model to the features of the dataset and then obtain the explained variance ratio using the explained_variance_ratio_ attribute.\n",
    "\n",
    "We also calculate the cumulative explained variance by taking the cumulative sum of the explained variance ratio. The cumulative explained variance tells us how much variance is explained by including each additional principal component.\n",
    "\n",
    "In this example, we choose to retain the principal components that explain at least 95% of the variance. We determine the number of principal components to retain by finding the index where the cumulative explained variance first exceeds or equals 0.95.\n",
    "\n",
    "Based on the output, we see that the first two principal components explain approximately 91.28% and 6.96% of the variance, respectively. The cumulative explained variance reaches 95% after the second principal component. Therefore, we choose to retain the first two principal components for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be1d98-28ed-4df8-ba8c-e7cd009b2de3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe86b6c-186c-4b48-baf4-1b58ba765404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
